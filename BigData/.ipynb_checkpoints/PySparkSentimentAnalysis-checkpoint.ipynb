{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "570a2b5a",
   "metadata": {},
   "source": [
    "\n",
    "# Sentiment Analysis in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a58e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, StopWordsRemover\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b648c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark context\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803adf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sc master-running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a4d5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Schema\n",
    "customSchema = StructType([\n",
    "    StructField(\"target\", StringType()),\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"flag\", StringType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"text\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe62f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the input file from Hadoop Distributed File System\n",
    "#http://help.sentiment140.com/for-students\n",
    "df = spark.read.load('hdfs://localhost:9000/user1/training.1600000.processed.noemoticon.csv', \n",
    "                     format=\"csv\", \n",
    "                     sep=',', \n",
    "                     schema=customSchema).toDF('target', 'id', 'date','flag','user','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b7bc619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|target|        id|                date|    flag|           user|                text|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "|     0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|     0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Display the first three records of our dataframe\n",
    "df.show(3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60bbdea",
   "metadata": {},
   "source": [
    "The table has 6 columns:\n",
    "<b>target</b>: contains the label of the sentiment.\n",
    "<b>id</b>: unique number for the tweet.\n",
    "<b>date</b>: tweet date\n",
    "<b>flag</b>: will not use, \n",
    "<b>user</b> twitter user's, <b>text</b> the actual tweet \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a49cdf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the number of rows in the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b271b1f",
   "metadata": {},
   "source": [
    "### Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8809b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the dataframe schema(in tree format)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c545ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename Columns\n",
    "# rename the text --> tweet \n",
    "df = df.withColumnRenamed(\"text\", \"tweet\")\n",
    "\n",
    "#rename the target --> Sentiment\n",
    "df = df.withColumnRenamed(\"target\", \"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a046652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+--------+---------------+--------------------+\n",
      "|Sentiment|        id|                date|    flag|           user|               tweet|\n",
      "+---------+----------+--------------------+--------+---------------+--------------------+\n",
      "|        0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|        0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "+---------+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92752659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the count of distinct values in the target column \n",
    "df.select('Sentiment').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "930ce542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Sentiment|\n",
      "+---------+\n",
      "|        0|\n",
      "|        4|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Display the unique distinct values in the target column \n",
    "df.select('Sentiment').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352eba40",
   "metadata": {},
   "source": [
    "The 'Sentiment' column has   two values of the sentiment (4) for positive tweet and (0) for negative tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d26ba0",
   "metadata": {},
   "source": [
    "#### Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01f36e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_null_cnt_df =  df.select([\n",
    "    count(when(col(c).isNull(),c)).alias(c) for c in df.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae90709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+----+----+-----+\n",
      "|Sentiment| id|date|flag|user|tweet|\n",
      "+---------+---+----+----+----+-----+\n",
      "|        0|  0|   0|   0|   0|    0|\n",
      "+---------+---+----+----+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "col_null_cnt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ed2f1",
   "metadata": {},
   "source": [
    "#### Indexing Dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5d738",
   "metadata": {},
   "source": [
    "We cannot access a Spark dataframe by [row,column] as we can a pandas dataframe since Spark dataframes are dispersed across clusters. A different approach to accomplishing that is by adding a new column with \"incremental ID\". Using the \".filter()\" function on the \"incremental ID\" column, we can then retrieve data by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8170b1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+--------+---------------+--------------------+-----+\n",
      "|Sentiment|        id|                date|    flag|           user|               tweet|index|\n",
      "+---------+----------+--------------------+--------+---------------+--------------------+-----+\n",
      "|        0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|    0|\n",
      "|        0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|    1|\n",
      "+---------+----------+--------------------+--------+---------------+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b114f17",
   "metadata": {},
   "source": [
    "#### Create a new dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d18ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new =  df.select(df[\"index\"],\n",
    "                             df[\"tweet\"],df[\"Sentiment\"].cast(\"Int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b69598b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+\n",
      "|index|               tweet|Sentiment|\n",
      "+-----+--------------------+---------+\n",
      "|    0|@switchfoot http:...|        0|\n",
      "|    1|is upset that he ...|        0|\n",
      "+-----+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76431b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = false)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- Sentiment: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the schema\n",
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f9bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91f5b02b",
   "metadata": {},
   "source": [
    "#### Split the data into train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27754550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1280082 ; Testing data rows: 319918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split the data, 80% for training, 20% for testing\n",
    "splitdata = df_new.randomSplit([0.8, 0.2]) \n",
    "data_train = splitdata[0]          #index 0 = data training\n",
    "data_test  = splitdata[1]          #index 1 = data testing\n",
    "train_rows = data_train.count()\n",
    "test_rows = data_test.count()\n",
    "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354244aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5273220e",
   "metadata": {},
   "source": [
    "#### Training Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad910c5",
   "metadata": {},
   "source": [
    "<b>Tokenizer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "373201da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+--------------------+\n",
      "|index|               tweet|Sentiment|      SentimentWords|\n",
      "+-----+--------------------+---------+--------------------+\n",
      "|    1|is upset that he ...|        0|[is, upset, that,...|\n",
      "|    2|@Kenichan I dived...|        0|[@kenichan, i, di...|\n",
      "|    3|my whole body fee...|        0|[my, whole, body,...|\n",
      "+-----+--------------------+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Separate \"SentimentText\" into individual words using tokenizer\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "# use PySparks build in tokenizer to tokenize tweets\n",
    "tokenizer = Tokenizer(inputCol  = \"tweet\",\n",
    "                      outputCol = \"SentimentWords\")\n",
    "\n",
    "tokenized_dfTrain = tokenizer.transform(data_train)\n",
    "tokenized_dfTrain.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b872e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<b>Remove Stop Words</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37a28365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+--------------------+--------------------+\n",
      "|index|               tweet|Sentiment|      SentimentWords|     MeaningfulWords|\n",
      "+-----+--------------------+---------+--------------------+--------------------+\n",
      "|    1|is upset that he ...|        0|[is, upset, that,...|[upset, update, f...|\n",
      "|    2|@Kenichan I dived...|        0|[@kenichan, i, di...|[@kenichan, dived...|\n",
      "|    3|my whole body fee...|        0|[my, whole, body,...|[whole, body, fee...|\n",
      "+-----+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenized_dfTrain)\n",
    "SwRemovedTrain.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf043f",
   "metadata": {},
   "source": [
    "<b>Converting words feature into numerical feature</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f1d085",
   "metadata": {},
   "source": [
    "In Spark 2.2.1,it is implemented in HashingTF funtion using Austin Appleby's MurmurHash 3 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fe75d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|Sentiment|     MeaningfulWords|            features|\n",
      "+---------+--------------------+--------------------+\n",
      "|        0|[upset, update, f...|(262144,[59577,61...|\n",
      "|        0|[@kenichan, dived...|(262144,[3924,283...|\n",
      "|        0|[whole, body, fee...|(262144,[34121,80...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numeric_dfTrain = hashTF.transform(SwRemovedTrain).select(\n",
    "    'Sentiment', 'MeaningfulWords', 'features')\n",
    "numeric_dfTrain.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf905d",
   "metadata": {},
   "source": [
    "### Train Our model using LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c859ea",
   "metadata": {},
   "source": [
    "#### Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2086ab8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 15:42:15,031 WARN memory.MemoryStore: Not enough space to cache rdd_87_1 in memory! (computed 65.0 MiB so far)\n",
      "2023-05-15 15:42:15,040 WARN storage.BlockManager: Persisting block rdd_87_1 to disk instead.\n",
      "2023-05-15 15:42:15,342 WARN memory.MemoryStore: Not enough space to cache rdd_87_0 in memory! (computed 65.0 MiB so far)\n",
      "2023-05-15 15:42:15,343 WARN storage.BlockManager: Persisting block rdd_87_0 to disk instead.\n",
      "2023-05-15 15:42:27,588 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2023-05-15 15:42:27,589 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"Sentiment\", featuresCol=\"features\", \n",
    "                        maxIter=10, regParam=0.01)\n",
    "model = lr.fit(numeric_dfTrain)\n",
    "print (\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565b763",
   "metadata": {},
   "source": [
    "### Prepare testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d831e028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|Sentiment|     MeaningfulWords|            features|\n",
      "+---------+--------------------+--------------------+\n",
      "|        0|[@switchfoot, htt...|(262144,[38640,52...|\n",
      "|        0|[@twittera, que, ...|(262144,[133107,1...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenized_dfTest = tokenizer.transform(data_test)\n",
    "SwRemovedTest = swr.transform(tokenized_dfTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select(\n",
    "    'Sentiment', 'MeaningfulWords', 'features')\n",
    "numericTest.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefa16d",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce7405c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 15:43:21,522 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 10.1 MiB\n",
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+\n",
      "|     MeaningfulWords|prediction|Sentiment|\n",
      "+--------------------+----------+---------+\n",
      "|[@switchfoot, htt...|       0.0|        0|\n",
      "|[@twittera, que, ...|       4.0|        0|\n",
      "|[@lettya, ahh, iv...|       0.0|        0|\n",
      "|[@angry_barista, ...|       0.0|        0|\n",
      "|[week, going, hoped]|       0.0|        0|\n",
      "|[thought, sleepin...|       0.0|        0|\n",
      "|[@fleurylis, eith...|       0.0|        0|\n",
      "|[really, feel, li...|       0.0|        0|\n",
      "|[checked, user, t...|       0.0|        0|\n",
      "|[broadband, plan,...|       0.0|        0|\n",
      "+--------------------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(numericTest)\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\", \"Sentiment\")\n",
    "predictionFinal.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff816cb",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c9683a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61b43adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 15:43:25,427 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 10.1 MiB\n",
      "[Stage 45:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 233568 , total data: 319918 , accuracy: 0.730087084815484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 45:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['Sentiment']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
    "      \", accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9de7f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 15:43:44,839 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 10.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73    160147\n",
      "           4       0.73      0.73      0.73    159771\n",
      "\n",
      "    accuracy                           0.73    319918\n",
      "   macro avg       0.73      0.73      0.73    319918\n",
      "weighted avg       0.73      0.73      0.73    319918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification Report\n",
    "y_true = predictionFinal.select(['Sentiment']).collect()\n",
    "y_pred = predictionFinal.select(['prediction']).collect()\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d151af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[116701  43446]\n",
      " [ 42904 116867]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "cm = print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4c42c",
   "metadata": {},
   "source": [
    "Reference:\n",
    "   https://github.com/ardianumam/compilations/blob/master/ApacheSparkVideoSeries/08%20Sentiment%20Analysis%20in%20Spark.ipynb\n",
    "   \n",
    "https://www.projectpro.io/recipes/get-null-count-of-each-column-of-dataframe-pyspark-databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04c00f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
